{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:01:33.092458Z",
     "start_time": "2019-07-17T11:01:00.601713Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, validation_curve, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV, PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.ensemble as ens\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "import sklearn.feature_selection\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, Normalizer, LabelEncoder, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:33.368681Z",
     "start_time": "2019-07-17T11:08:33.253769Z"
    }
   },
   "outputs": [],
   "source": [
    "labelled = pd.read_csv('../input/train.csv') # Labelled Data for training, validation, and model assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:34.776564Z",
     "start_time": "2019-07-17T11:08:34.707616Z"
    }
   },
   "outputs": [],
   "source": [
    "unlabelled = pd.read_csv('../input/test.csv') # Unlabelled Data for final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:36.180783Z",
     "start_time": "2019-07-17T11:08:36.174788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep PassengerId for final submission in seperate variable.\n",
    "passngerID = unlabelled[['PassengerId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate both labelled and unlabelled data so that all data cleaning and feature engineering will applied to both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:39.500065Z",
     "start_time": "2019-07-17T11:08:39.489060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([labelled, unlabelled], axis= 0, sort= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap= 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fare column has only one null value.<br/>\n",
    "- Age column has many null values.<br/>\n",
    "- Cabin column has a majority of null values.<br/>\n",
    "- Survived column has null values for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data = data, x= 'Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is the most survived gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = data, x= 'Survived', hue= 'Sex')\n",
    "plt.legend(loc =(1.1,0.9)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does first class have more survival rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data = data, x='Survived', hue='Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distribution of passengers' age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Age'].dropna(), kde = False, bins = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distribution of number of siblings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'SibSp', data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of passenger's in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data= data.dropna(), x='Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of each gender in different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data= data, x='Pclass', hue= 'Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket fare for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data= data.dropna(), x='Pclass', y= 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values in Age with the median age for the corresponding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:49.105740Z",
     "start_time": "2019-07-17T11:08:49.090751Z"
    }
   },
   "outputs": [],
   "source": [
    "class_mean_age = data.pivot_table(values='Age', index='Pclass', aggfunc='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:50.175068Z",
     "start_time": "2019-07-17T11:08:50.170071Z"
    }
   },
   "outputs": [],
   "source": [
    "null_age = data['Age'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:51.850230Z",
     "start_time": "2019-07-17T11:08:51.768284Z"
    }
   },
   "outputs": [],
   "source": [
    "data.loc[null_age,'Age'] = data.loc[null_age,'Pclass'].apply(lambda x: class_mean_age.loc[x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:53.502218Z",
     "start_time": "2019-07-17T11:08:53.494225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Age.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the missing value in Fare with the median fare for the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:55.847277Z",
     "start_time": "2019-07-17T11:08:55.832290Z"
    }
   },
   "outputs": [],
   "source": [
    "class_mean_fare = data.pivot_table(values= 'Fare', index= 'Pclass', aggfunc='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:57.090352Z",
     "start_time": "2019-07-17T11:08:57.084359Z"
    }
   },
   "outputs": [],
   "source": [
    "null_fare = data['Fare'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:58.390252Z",
     "start_time": "2019-07-17T11:08:58.377260Z"
    }
   },
   "outputs": [],
   "source": [
    "data.loc[null_fare, 'Fare'] = data.loc[null_fare, 'Pclass'].apply(lambda x: class_mean_fare.loc[x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:08:59.329459Z",
     "start_time": "2019-07-17T11:08:59.323464Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Fare.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing the missing values in Embarked with the most common port for corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:01.607655Z",
     "start_time": "2019-07-17T11:09:01.598667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S    914\n",
       "C    270\n",
       "Q    123\n",
       "Name: Embarked, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:03.618902Z",
     "start_time": "2019-07-17T11:09:03.613893Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Embarked'] = data.Embarked.fillna('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:04.763236Z",
     "start_time": "2019-07-17T11:09:04.755242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Embarked.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Create a new feature with the title of each passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:15.104739Z",
     "start_time": "2019-07-17T11:09:15.098744Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Title'] = data.Name.apply(lambda x : x[x.find(',')+2:x.find('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:16.566793Z",
     "start_time": "2019-07-17T11:09:16.557799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr              757\n",
       "Miss            260\n",
       "Mrs             197\n",
       "Master           61\n",
       "Dr                8\n",
       "Rev               8\n",
       "Col               4\n",
       "Major             2\n",
       "Ms                2\n",
       "Mlle              2\n",
       "Capt              1\n",
       "Sir               1\n",
       "Jonkheer          1\n",
       "the Countess      1\n",
       "Dona              1\n",
       "Don               1\n",
       "Mme               1\n",
       "Lady              1\n",
       "Name: Title, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Title.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that only 4 titles have significant frequency and the others are repeated only 8 time or less.<br/> So, we will combine all titles with small frequency under one title (say, Other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:18.597496Z",
     "start_time": "2019-07-17T11:09:18.591504Z"
    }
   },
   "outputs": [],
   "source": [
    "rare_titles = (data['Title'].value_counts() < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:19.640737Z",
     "start_time": "2019-07-17T11:09:19.573772Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Title'] = data['Title'].apply(lambda x : 'Other' if rare_titles.loc[x] == True else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new feature for the family size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature combines the number of siblings and parents/children (SibSp and Parch) +1 (The passenger himself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:22.950610Z",
     "start_time": "2019-07-17T11:09:22.659083Z"
    }
   },
   "outputs": [],
   "source": [
    "data['FamilySize'] = data['SibSp'] + data['Parch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new feature to indicate whether the passenger was alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:25.583207Z",
     "start_time": "2019-07-17T11:09:25.579210Z"
    }
   },
   "outputs": [],
   "source": [
    "data['IsAlone'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:26.705936Z",
     "start_time": "2019-07-17T11:09:26.647984Z"
    }
   },
   "outputs": [],
   "source": [
    "data['IsAlone'].loc[ data['FamilySize'] == 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new feature by discretizing Age into buckets/bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age is discretized into 4 bins coresponding to 4 stages of human life:<br/>\n",
    "1. Childhood.\n",
    "2. Adolescence.\n",
    "3. Adulthood.\n",
    "4. Old Age. <br/>\n",
    "Check this link for more details: https://bit.ly/2LkPFPf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:29.975175Z",
     "start_time": "2019-07-17T11:09:29.970168Z"
    }
   },
   "outputs": [],
   "source": [
    "data['AgeBins'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:31.320828Z",
     "start_time": "2019-07-17T11:09:31.173945Z"
    }
   },
   "outputs": [],
   "source": [
    "data['AgeBins'].loc[(data['Age'] >= 11) & (data['Age'] < 20)] = 1\n",
    "data['AgeBins'].loc[(data['Age'] >= 20) & (data['Age'] < 60)] = 2\n",
    "data['AgeBins'].loc[data['Age'] >= 60] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new feature by discretizing Fare into 4 buckets/bins based on quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:34.917401Z",
     "start_time": "2019-07-17T11:09:34.899415Z"
    }
   },
   "outputs": [],
   "source": [
    "data['FareBins'] = pd.qcut(data['Fare'], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unused columns from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Some features are expected to not have effect of the classification such as PassengerId, Name and Ticket. <br/> \n",
    "2. Also some futures have too much missing values such as the Cabin which render it useless.\n",
    "3. We'll also drop the original features we used to create the new features because there will be high correlation between these features which may confuse the model about feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:38.938804Z",
     "start_time": "2019-07-17T11:09:38.931809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Title', 'FamilySize',\n",
       "       'IsAlone', 'AgeBins', 'FareBins'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:40.461058Z",
     "start_time": "2019-07-17T11:09:40.456061Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(columns=['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare', 'SibSp', 'Parch'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert qualitative features into numeric form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features (Embarked, Sex, Title) to numerical features and drop one dummy variable for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:45.538700Z",
     "start_time": "2019-07-17T11:09:45.522701Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(\n",
    "    data, columns=['Embarked', 'Sex', 'Title'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert qualitative ordinal features (FareBins) into numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:48.103372Z",
     "start_time": "2019-07-17T11:09:48.092385Z"
    }
   },
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "data['FareBins'] = label.fit_transform(data['FareBins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:49.071817Z",
     "start_time": "2019-07-17T11:09:48.957906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>AgeBins</th>\n",
       "      <th>FareBins</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  FamilySize  IsAlone  AgeBins  FareBins  Embarked_Q  \\\n",
       "0       0.0       3           2        0        2         0           0   \n",
       "1       1.0       1           2        0        2         3           0   \n",
       "2       1.0       3           1        1        2         1           0   \n",
       "3       1.0       1           2        0        2         3           0   \n",
       "4       0.0       3           1        1        2         1           0   \n",
       "5       0.0       3           1        1        2         1           1   \n",
       "6       0.0       1           1        1        2         3           0   \n",
       "\n",
       "   Embarked_S  Sex_male  Title_Miss  Title_Mr  Title_Mrs  Title_Other  \n",
       "0           1         1           0         1          0            0  \n",
       "1           0         0           0         0          1            0  \n",
       "2           1         0           1         0          0            0  \n",
       "3           1         0           0         0          1            0  \n",
       "4           1         1           0         1          0            0  \n",
       "5           0         1           0         1          0            0  \n",
       "6           1         1           0         1          0            0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data back to labelled/unlabelled sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important step before scaling features. Since the scaler should be fit on the training set only and then applied to both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:52.721753Z",
     "start_time": "2019-07-17T11:09:52.707779Z"
    }
   },
   "outputs": [],
   "source": [
    "labelled = data[data.Survived.isnull() == False].reset_index(drop=True)\n",
    "unlabelled = data[data.Survived.isnull()].drop(columns = ['Survived']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:55.572607Z",
     "start_time": "2019-07-17T11:09:55.566613Z"
    }
   },
   "outputs": [],
   "source": [
    "labelled['Survived'] = labelled.Survived.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling features using different scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try the following scalers on a copy of the original data frame and we'll select the best one:\n",
    "1. MinMaxScaler\n",
    "2. MaxAbsScaler\n",
    "3. StandardScaler\n",
    "4. RobustScaler\n",
    "5. Normalizer\n",
    "6. QuantileTransformer\n",
    "7. PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:09:59.965167Z",
     "start_time": "2019-07-17T11:09:59.961170Z"
    }
   },
   "outputs": [],
   "source": [
    "scalers = [MinMaxScaler(), MaxAbsScaler(), StandardScaler(), RobustScaler(),\n",
    "            Normalizer(), QuantileTransformer(), PowerTransformer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:10:09.941846Z",
     "start_time": "2019-07-17T11:10:02.646874Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler_score = {}\n",
    "labelled_copy = labelled.copy(deep= True) # Creat a copy of the original Labelled DF.\n",
    "for scaler in scalers:\n",
    "    scaler.fit(labelled_copy[['FamilySize']])\n",
    "    labelled_copy['FamilySize'] = scaler.transform(labelled_copy[['FamilySize']])\n",
    "    lr = LogisticRegressionCV(cv = 10, scoring= 'accuracy')\n",
    "    lr.fit(labelled_copy.drop(columns=['Survived']), labelled_copy.Survived)\n",
    "    score = lr.score(labelled_copy.drop(columns=['Survived']), labelled_copy.Survived)\n",
    "    scaler_score.update({scaler:score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:10:11.910667Z",
     "start_time": "2019-07-17T11:10:11.903677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{MinMaxScaler(copy=True, feature_range=(0, 1)): 0.8305274971941639,\n",
       " MaxAbsScaler(copy=True): 0.8305274971941639,\n",
       " StandardScaler(copy=True, with_mean=True, with_std=True): 0.8305274971941639,\n",
       " RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "        with_scaling=True): 0.8305274971941639,\n",
       " Normalizer(copy=True, norm='l2'): 0.8080808080808081,\n",
       " QuantileTransformer(copy=True, ignore_implicit_zeros=False, n_quantiles=1000,\n",
       "           output_distribution='uniform', random_state=None,\n",
       "           subsample=100000): 0.8080808080808081,\n",
       " PowerTransformer(copy=True, method='yeo-johnson', standardize=True): 0.8058361391694725}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that MinMaxScaler, MaxAbsScaler, StandardScaler, and RobustScaler results in the same accuracy score. So, we will use the StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:10:22.172381Z",
     "start_time": "2019-07-17T11:10:22.152399Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(labelled[['FamilySize']])\n",
    "labelled['FamilySize'] = scaler.transform(labelled[['FamilySize']])\n",
    "unlabelled['FamilySize'] = scaler.transform(unlabelled[['FamilySize']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation/Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the labelled data into 3 sets:\n",
    "1. Training set: used for model training. (Size = %70)\n",
    "2. Validation set: used for hyperparameter tunning. (Size = %15)\n",
    "3. Test set: used for model assessment and comparison of different models. (Size = %15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform data split on two steps using train_test_split function:\n",
    "   1. we split data into training set and other set.\n",
    "   2. we split the other set into validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:10:42.201170Z",
     "start_time": "2019-07-17T11:10:42.189164Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_other, y_train, y_other = train_test_split(\n",
    "                labelled.drop(columns=['Survived']), labelled.Survived, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:10:43.497915Z",
     "start_time": "2019-07-17T11:10:43.488925Z"
    }
   },
   "outputs": [],
   "source": [
    "x_valid, x_test, y_valid, y_test = train_test_split(\n",
    "                                    x_other, y_other, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features/Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will seperate the features and target columns from the label data so that it can be used in the feature selection step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T11:10:46.359058Z",
     "start_time": "2019-07-17T11:10:46.353063Z"
    }
   },
   "outputs": [],
   "source": [
    "features = labelled.drop(columns=['Survived'])\n",
    "target = labelled.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(\n",
    "    features, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=kf)\n",
    "nb = GaussianNB()\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=14, leaf_size=20, p=1, weights='uniform')\n",
    "svm = SVC(kernel='rbf', gamma=0.1, degree=1, C=500, shrinking=True)\n",
    "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5)\n",
    "adab = AdaBoostClassifier(n_estimators=500, learning_rate=0.7)\n",
    "bg = BaggingClassifier(n_estimators=100)\n",
    "gboost = GradientBoostingClassifier(\n",
    "    validation_fraction=0.1, n_iter_no_change=20, tol=0.005)\n",
    "xgboost = xgb.XGBClassifier()\n",
    "lgboost = lgb.LGBMClassifier()\n",
    "ctboost = ctb.CatBoostClassifier(iterations=200, learning_rate=0.1, depth=10)\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "nb.fit(x_train, y_train)\n",
    "knn.fit(x_train, y_train)\n",
    "svm.fit(x_train, y_train)\n",
    "gb.fit(x_train, y_train)\n",
    "adab.fit(x_train, y_train)\n",
    "bg.fit(x_train, y_train)\n",
    "gboost.fit(x_train, y_train)\n",
    "xgboost.fit(x_train, y_train)\n",
    "lgboost.fit(x_train, y_train)\n",
    "ctboost.fit(x_train, y_train)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = lr.predict(x_validate)\n",
    "pred2 = nb.predict(x_validate)\n",
    "pred3 = knn.predict(x_validate)\n",
    "pred4 = svm.predict(x_validate)\n",
    "pred5 = gb.predict(x_validate)\n",
    "pred6 = adab.predict(x_validate)\n",
    "pred7 = bg.predict(x_validate)\n",
    "pred8 = gboost.predict(x_validate)\n",
    "pred9 = xgboost.predict(x_validate)\n",
    "pred10 = lgboost.predict(x_validate)\n",
    "pred11 = ctboost.predict(x_validate)\n",
    "pred12 = rf.predict(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred1 = lr.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred2 = nb.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred3 = knn.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred4 = svm.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred5 = gb.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred6 = adab.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred7 = bg.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred8 = gboost.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred9 = xgboost.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred10 = lgboost.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred11 = ctboost.predict(test.drop(columns=['PassengerId']))\n",
    "test_pred12 = rf.predict(test.drop(columns=['PassengerId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = np.column_stack((pred1, pred2, pred3, pred4, pred5, pred6, pred7,\n",
    "                                       pred8, pred9, pred10, pred11, pred12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_test_predictions = np.column_stack((test_pred1, test_pred2, test_pred3, test_pred4, test_pred5,\n",
    "                                            test_pred6, test_pred7, test_pred8, test_pred9, test_pred10,\n",
    "                                            test_pred11, test_pred12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta model\n",
    "meta_model = LogisticRegressionCV(cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_model.fit(stacked_predictions, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stack = pd.DataFrame(meta_model.predict(\n",
    "    stacked_test_predictions), columns=['Survived'], dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_model = pd.concat([test.PassengerId, y_pred_stack], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
